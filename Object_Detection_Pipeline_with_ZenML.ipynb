{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlYjfaGHem2V"
   },
   "source": [
    "# ML Pipelines with ZenML\n",
    "\n",
    "***Key Concepts:*** *ML Pipelines, Steps*\n",
    "\n",
    "In this notebook, we will learn how to easily convert existing ML code into ML pipelines using ZenML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV4LULRKNYRN"
   },
   "source": [
    "Machine learning in production consists of wide variety of tasks ranging from experiment tracking to orchestration, from model deployment to monitoring, from drift detection to feature stores and much, much more than that. Even though there are already some seemingly well-established solutions for these tasks, it can become increasingly difficult to establish a running production system in a reliable and modular manner once all these solutions are brought together. This is a problem which is especially critical when switching from research setting to a production setting. Due to a lack of standards, the time and resources invested in proof of concepts frequently go completely to waste, because the initial system cannot easily be transferred to a production-grade setting. \n",
    "\n",
    "To solve the above challenging problem, Zen ML was introduced. This has got a set of standards and well-structured abstractions. It is essential that these abstractions not only cover concepts such as pipelines and steps but also the infrastructure elements on which the pipelines run. This helps to simply infrastructure configuration and management. ZenML is a framework to create reproducible, production-ready machine learning pipelines. It is built for data scientist to transition their models from a local experimental setup to a robust modern MLOPS infrastructure in production.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T__4ElNem2c"
   },
   "source": [
    "Since we will build models with [sklearn](https://scikit-learn.org/stable/), you will need to have the ZenML sklearn integration installed. You can install ZenML and the sklearn integration with the following command, which will also restart the kernel of your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hUe3ay8Oem2d",
    "outputId": "e660b327-4e59-4818-a00f-700ec3219f7f"
   },
   "outputs": [],
   "source": [
    "%pip install zenml\n",
    "!zenml integration install sklearn -y\n",
    "%pip install pyparsing==2.4.2  # required for Colab\n",
    "\n",
    "import IPython\n",
    "\n",
    "# automatically restart kernel\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jlz9rV--em2i"
   },
   "source": [
    "As an ML practitioner, you are probably familiar with building ML models using Scikit-learn, PyTorch, TensorFlow, or similar. An **[ML Pipeline](https://docs.zenml.io/developer-guide/steps-and-pipelines)** is simply an extension, including other steps you would typically do before or after building a model, like data acquisition, preprocessing, model deployment, or monitoring. The ML pipeline essentially defines a step-by-step procedure of your work as an ML practitioner. Defining ML pipelines explicitly in code is great because:\n",
    "- We can easily rerun all of our work, not just the model, eliminating bugs and making our models easier to reproduce.\n",
    "- Data and models can be versioned and tracked, so we can see at a glance which dataset a model was trained on and how it compares to other models.\n",
    "- If the entire pipeline is coded up, we can automate many operational tasks, like retraining and redeploying models when the underlying problem or data changes or rolling out new and improved models with CI/CD workflows.\n",
    "\n",
    "Having a clearly defined ML pipeline is essential for ML teams that aim to serve models on a large scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTAuQH7mem2j"
   },
   "source": [
    "## ZenML Setup\n",
    "Throughout this series, we will define our ML pipelines using [ZenML](https://github.com/zenml-io/zenml/). ZenML is an excellent tool for this task, as it is straightforward and intuitive to use and has [integrations](https://docs.zenml.io/mlops-stacks/integrations) with most of the advanced MLOps tools we will want to use later. Make sure you have ZenML installed (via `pip install zenml`). Let's run some commands to make sure you start with a fresh ML stack. You can ignore the details for now, as we will learn about it in more detail in a later chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vrwm0lUYem2k"
   },
   "outputs": [],
   "source": [
    "!rm -rf .zen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TdDLj4lnYb5"
   },
   "source": [
    "# Initialize zenML repository:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efaE723gwErL"
   },
   "source": [
    "Below command will internally create a local directory with a bunch of configuration for your MLOPs stack. Stacks represent different configurations of MLOps tools and infrastructure; Each stack consists of multiple Stack Components that each come in several Flavors. The default local stack will be 'default'. This local configuration will only take effect when youâ€™re running ZenML from the initialized repository root, or from a subdirectory. The default stack consists of  \n",
    "\n",
    "- Orchestrator: This is essentially your python kernel. \n",
    "\n",
    "- Artifact store: This store all the artifacts that flow through between steps \n",
    "\n",
    "- Metadata store: This keeps tracks of all the parameters that flow through your pipeline. \n",
    "\n",
    "Repositories link stacks to the pipeline and step code of your ML projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NTcd4DdDnVOX",
    "outputId": "2053f2c0-db02-40d3-fcf0-a6acdf309908"
   },
   "outputs": [],
   "source": [
    "!zenml init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znb0IB7ayJ8P"
   },
   "source": [
    "Profiles manage these stacks and enable having various ZenML configurations on the same machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hvg4B7KrngaU",
    "outputId": "66742c94-9601-47bc-9a8d-a7871ad275fc"
   },
   "outputs": [],
   "source": [
    "!zenml profile create zenbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3O_YxUU7oFpz",
    "outputId": "fe9b71aa-edcd-41f0-fa5a-5a4a87cd0d8a"
   },
   "outputs": [],
   "source": [
    "!zenml profile set zenbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfJb-riFoHGd",
    "outputId": "d3d9bc91-805f-42df-d57f-a17a91864e61"
   },
   "outputs": [],
   "source": [
    "!zenml stack set default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33Jp9j9-oLTG",
    "outputId": "78fc86b1-73cf-42d0-c31d-e0c5c6da1367"
   },
   "outputs": [],
   "source": [
    "!zenml stack get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGvNB52bem2k"
   },
   "source": [
    "## Example Experimentation ML Code\n",
    "Let us get started with some simple exemplary ML code. In the following, we train a Scikit-learn SVC classifier to classify images of handwritten digits. We load the data, train a model on the training set, then test it on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMjRX638yno4"
   },
   "source": [
    "Let's first do the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vsriniva\\Desktop\\object_detection\\yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (3.3.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.20.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (4.5.3.56)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (8.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (5.4.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (2.25.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (1.6.2)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (1.12.1)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (0.13.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (4.64.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (3.19.4)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (2.8.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 13)) (1.2.4)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (0.11.1)\n",
      "Requirement already satisfied: ipython in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 15)) (7.22.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 16)) (5.8.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 17)) (0.1.1.post2207130030)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 1)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 6)) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 6)) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 6)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 6)) (4.0.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from torch>=1.7.0->-r requirements.txt (line 8)) (3.10.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->-r requirements.txt (line 10)) (0.4.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 12)) (1.44.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 12)) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 12)) (1.35.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 12)) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 12)) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 12)) (0.11.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 12)) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 12)) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 12)) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 12)) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->-r requirements.txt (line 13)) (2021.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from ipython->-r requirements.txt (line 15)) (0.17.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from ipython->-r requirements.txt (line 15)) (5.0.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from ipython->-r requirements.txt (line 15)) (4.4.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from ipython->-r requirements.txt (line 15)) (0.2.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from ipython->-r requirements.txt (line 15)) (2.8.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from ipython->-r requirements.txt (line 15)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from ipython->-r requirements.txt (line 15)) (3.0.17)\n",
      "Requirement already satisfied: six in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r requirements.txt (line 12)) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 12)) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 12)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 12)) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 12)) (1.3.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython->-r requirements.txt (line 15)) (0.7.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 12)) (4.11.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 15)) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython->-r requirements.txt (line 15)) (0.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 12)) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 12)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\vsriniva\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 12)) (3.2.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt --proxy  http://approxy.rockwellcollins.com:9090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=dataset.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=2, batch_size=1, imgsz=320, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "fatal: unable to access 'https://github.com/ultralytics/yolov5/': Could not resolve proxy: approxy.rockwellcollins.com\n",
      "YOLOv5  v6.2-53-gf0e5a60 Python-3.8.8 torch-1.12.1+cpu CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5  runs in Weights & Biases\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5  in ClearML\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\n",
      "2022-08-25 15:21:54.743733: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-08-25 15:21:54.743763: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 270 layers, 7030417 parameters, 7030417 gradients, 16.0 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'C:\\Users\\vsriniva\\Desktop\\object_detection\\data\\labels\\train.cache' images and labels... 780 found, 0 missing, 0 empty, 0 corrupt: 100%|##########| 780/780 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'C:\\Users\\vsriniva\\Desktop\\object_detection\\data\\labels\\train.cache' images and labels... 780 found, 0 missing, 0 empty, 0 corrupt: 100%|##########| 780/780 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/780 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):   1%|1         | 11/780 [00:00<00:07, 98.98it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):   4%|3         | 28/780 [00:00<00:05, 135.12it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):   6%|5         | 44/780 [00:00<00:05, 141.08it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):   8%|8         | 63/780 [00:00<00:04, 151.53it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  10%|#         | 79/780 [00:00<00:04, 150.33it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  12%|#2        | 97/780 [00:00<00:04, 150.21it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  15%|#4        | 114/780 [00:00<00:04, 152.94it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  17%|#6        | 130/780 [00:00<00:04, 151.01it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  19%|#8        | 146/780 [00:00<00:04, 150.55it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  21%|##1       | 165/780 [00:01<00:03, 154.38it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  23%|##3       | 181/780 [00:01<00:03, 155.53it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  26%|##5       | 200/780 [00:01<00:03, 156.87it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  28%|##7       | 218/780 [00:01<00:03, 153.87it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  30%|###       | 235/780 [00:01<00:03, 155.31it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  32%|###2      | 253/780 [00:01<00:03, 154.45it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  35%|###4      | 270/780 [00:01<00:03, 156.56it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  37%|###6      | 287/780 [00:01<00:03, 153.88it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  39%|###8      | 303/780 [00:02<00:03, 149.38it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  41%|####      | 319/780 [00:02<00:03, 149.17it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  43%|####3     | 337/780 [00:02<00:02, 154.73it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  45%|####5     | 353/780 [00:02<00:02, 154.47it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  47%|####7     | 369/780 [00:02<00:02, 155.16it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  50%|####9     | 388/780 [00:02<00:02, 164.69it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  52%|#####1    | 405/780 [00:02<00:02, 164.32it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  54%|#####4    | 422/780 [00:02<00:02, 155.57it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  56%|#####6    | 439/780 [00:02<00:02, 157.85it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  59%|#####8    | 458/780 [00:02<00:01, 166.49it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  61%|######    | 475/780 [00:03<00:01, 165.59it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  63%|######3   | 492/780 [00:03<00:01, 161.28it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  66%|######5   | 511/780 [00:03<00:01, 164.77it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  68%|######7   | 530/780 [00:03<00:01, 164.02it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  70%|#######   | 547/780 [00:03<00:01, 149.43it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  72%|#######2  | 565/780 [00:03<00:01, 156.68it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  74%|#######4  | 581/780 [00:03<00:01, 149.82it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  77%|#######6  | 600/780 [00:03<00:01, 160.25it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  79%|#######9  | 617/780 [00:03<00:01, 161.17it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  81%|########1 | 634/780 [00:04<00:00, 161.59it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  83%|########3 | 651/780 [00:04<00:00, 160.33it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  86%|########5 | 668/780 [00:04<00:00, 156.83it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  88%|########7 | 684/780 [00:04<00:00, 154.26it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  90%|########9 | 700/780 [00:04<00:00, 150.77it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  92%|#########1| 716/780 [00:04<00:00, 151.24it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  94%|#########3| 732/780 [00:04<00:00, 146.27it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  96%|#########6| 749/780 [00:04<00:00, 151.18it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  98%|#########8| 766/780 [00:04<00:00, 148.93it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100%|##########| 780/780 [00:05<00:00, 155.59it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'C:\\Users\\vsriniva\\Desktop\\object_detection\\data\\labels\\val.cache' images and labels... 260 found, 0 missing, 0 empty, 0 corrupt: 100%|##########| 260/260 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'C:\\Users\\vsriniva\\Desktop\\object_detection\\data\\labels\\val.cache' images and labels... 260 found, 0 missing, 0 empty, 0 corrupt: 100%|##########| 260/260 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/260 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):   4%|3         | 10/260 [00:00<00:02, 89.28it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  10%|#         | 26/260 [00:00<00:01, 117.11it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  16%|#6        | 42/260 [00:00<00:01, 130.76it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  22%|##2       | 58/260 [00:00<00:01, 137.97it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  28%|##8       | 74/260 [00:00<00:01, 137.51it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  35%|###4      | 90/260 [00:00<00:01, 140.17it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  41%|####      | 106/260 [00:00<00:01, 139.29it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  47%|####6     | 122/260 [00:00<00:00, 140.82it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  53%|#####3    | 138/260 [00:01<00:00, 140.22it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  59%|#####9    | 154/260 [00:01<00:00, 142.98it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  65%|######5   | 170/260 [00:01<00:00, 143.33it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  72%|#######1  | 186/260 [00:01<00:00, 143.19it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  78%|#######7  | 202/260 [00:01<00:00, 146.67it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  84%|########3 | 218/260 [00:01<00:00, 149.61it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  90%|######### | 234/260 [00:01<00:00, 147.51it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  96%|#########6| 250/260 [00:01<00:00, 145.28it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100%|##########| 260/260 [00:01<00:00, 144.09it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.69 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n",
      "Plotting labels to runs\\train\\exp6\\labels.jpg... \n",
      "OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.\n",
      "OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command 'git fetch origin' returned non-zero exit status 128.\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 320 --batch 1 --epochs 2 --data dataset.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZXm3B5Dem2p"
   },
   "source": [
    "## Turning experiments into ML pipelines with ZenML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzEl4UZtem2q"
   },
   "source": [
    "In ZenML, all the things can be defined as functions using the functional API. All you must do is to define a function, define its inputs, define its output, and then write python code in the middle. You just need to decorate that function with step decorator which you import from ZenML. Steps are the atomic components of a ZenML pipeline. Each step is defined by its inputs, the logic it applies and its outputs. \n",
    "\n",
    "For simple illustrations, assume your ML workflow contains data loading, model training, and model evaluation. In practice, your ML workflows will, of course, be much more complicated than that. You might have complex preprocessing that you do not want to redo every time you train a model, you will need to compare the performance of different models, deploy them in a production setting, and much more. Here ML pipelines come into play, allowing us to define our workflows in modular steps that we can then mix and match.\n",
    "\n",
    "![Digits Pipeline](https://github.com/zenml-io/zenbytes/blob/main/_assets/1-1/digits_pipeline.png?raw=1)\n",
    "\n",
    "We can identify three distinct steps in our example: data loading, model training, and model evaluation. Let us now define each of them as a ZenML **[Pipeline Step](https://docs.zenml.io/developer-guide/steps-and-pipelines#step)** simply by moving each step to its own function and decorating them with ZenML's `@step` [Python decorator](https://realpython.com/primer-on-python-decorators/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He46hRVzJUu8"
   },
   "source": [
    "Steps are the atomic components of a ZenML pipeline. Each step is defined by its inputs, the logic it applies and its outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3JHyD7Aem2r"
   },
   "outputs": [],
   "source": [
    "from zenml.steps import step, Output\n",
    "\n",
    "@step\n",
    "def importer() -> Output(\n",
    "    X_train=np.ndarray,\n",
    "    X_test=np.ndarray,\n",
    "    y_train=np.ndarray,\n",
    "    y_test=np.ndarray,\n",
    "):\n",
    "    \"\"\"Load the digits dataset as numpy arrays.\"\"\"\n",
    "    digits = load_digits()\n",
    "    data = digits.images.reshape((len(digits.images), -1))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data, digits.target, test_size=0.2, shuffle=False\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RvUiLsaKLMf"
   },
   "source": [
    "As this step has multiple outputs, we need to use the zenml.steps.step_output.Output class to indicate the names of each output. These names can be used to directly access the outputs of steps after running a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amGFo6aqKh-C"
   },
   "source": [
    "Let's come up with a second step that consumes the output of our first step and performs some sort of transformation on it. In this case, let's train a support vector machine classifier on the training data using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_Thq5L4KIK5"
   },
   "outputs": [],
   "source": [
    "@step\n",
    "def svc_trainer(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train a sklearn SVC classifier.\"\"\"\n",
    "    model = SVC(gamma=0.001)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTYLbjJTKKBb"
   },
   "outputs": [],
   "source": [
    "@step\n",
    "def evaluator(\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    model: ClassifierMixin,\n",
    ") -> float:\n",
    "    \"\"\"Calculate the test set accuracy of an sklearn model.\"\"\"\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn1rxYIIem26"
   },
   "source": [
    "Similarly, we can use ZenML's `@pipeline` decorator to connect all of our steps into an ML pipeline.This is agnostic of the implementation and can be done by routing outputs through the steps within the pipeline.\n",
    "\n",
    "Note that the pipeline definition does not depend on the concrete step functions we defined above; it merely establishes a recipe for how data moves through the steps. This means we can replace steps as we wish, e.g., to run the same pipeline with different models to compare their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFyQ5rCwem27"
   },
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def digits_pipeline(importer, trainer, evaluator):\n",
    "    \"\"\"Links all the steps together in a pipeline\"\"\"\n",
    "    X_train, X_test, y_train, y_test = importer()\n",
    "    model = trainer(X_train=X_train, y_train=y_train)\n",
    "    evaluator(X_test=X_test, y_test=y_test, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D_rUP_rKwxb"
   },
   "source": [
    "In case you want to run the step function outside the context of a ZenML pipeline, all you need to do is call the .entrypoint() method with the same input signature. For example:\n",
    "trainer.entrypoint(X_train=..., y_train=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft8bSh_-em28"
   },
   "source": [
    "## Running ZenML Pipelines\n",
    "Finally, we initialize our pipeline with concrete step functions and call the `run()` method to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mf4B0nssMBJF"
   },
   "source": [
    "With your pipeline recipe in hand you can now specify which concrete step implementations to use when instantiating the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOwwhjTEem28"
   },
   "outputs": [],
   "source": [
    "digits_svc_pipeline = digits_pipeline(\n",
    "    importer=importer(), trainer=svc_trainer(), evaluator=evaluator()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiJ0TrRoMG5-"
   },
   "source": [
    "Currently, you cannot use the same step twice in a pipeline because step names must be unique. If you would like to reuse a step, use the clone_step() utility function to create a copy of the step with a new name.\n",
    "\n",
    "To give each pipeline run a name:\n",
    "When running a pipeline by calling my_pipeline.run(), ZenML uses the current date and time as the name for the pipeline run. In order to change the name for a run, pass run_name as a parameter to the run() function:\n",
    "\n",
    "pipeline_instance.run(run_name=\"custom_pipeline_run_name\")\n",
    "\n",
    "Pipeline run names must be unique, so make sure to compute it dynamically if you plan to run your pipeline multiple times.\n",
    "\n",
    "You can then execute your pipeline instance with the .run() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6ud2W3XL44A",
    "outputId": "f843bd71-3a48-470b-fb04-99bc4348c46e"
   },
   "outputs": [],
   "source": [
    "digits_svc_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5pTnywxem29"
   },
   "source": [
    "And that's it, we just built our first ML pipeline! Great job!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML_Pipeline_with_ZenML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f70ec6e6bd16014ded89c8222361cbe53cd9507d51ebdcdf3ab6e494d45cf74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
